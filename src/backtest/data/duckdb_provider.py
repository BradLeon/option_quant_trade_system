"""
DuckDB Data Provider - å›æµ‹æ•°æ®æä¾›è€…

ä»æœ¬åœ° DuckDB/Parquet æ–‡ä»¶è¯»å–å†å²æ•°æ®ï¼Œå®ç°ä¸ IBKRProvider/FutuProvider ç›¸åŒçš„æ¥å£ã€‚
è¿™æ˜¯å›æµ‹ç³»ç»Ÿçš„æ ¸å¿ƒæ•°æ®æºã€‚

å…³é”®ç‰¹æ€§:
- å®ç° DataProvider æ¥å£ï¼Œä¸å®ç›˜ä»£ç æ— ç¼åˆ‡æ¢
- as_of_date: å›æµ‹å½“å‰æ—¥æœŸï¼Œæ‰€æœ‰æŸ¥è¯¢åªè¿”å›è¯¥æ—¥æœŸæˆ–ä¹‹å‰çš„æ•°æ®
- æ”¯æŒä» Parquet ç›´æ¥è¯»å–æˆ–é€šè¿‡ DuckDB æŸ¥è¯¢

Usage:
    provider = DuckDBProvider(
        data_dir="/Volumes/TradingData/processed",
        as_of_date=date(2024, 1, 15),
    )

    # ä¸ IBKRProvider å®Œå…¨ç›¸åŒçš„æ¥å£
    quote = provider.get_stock_quote("AAPL")
    chain = provider.get_option_chain("AAPL", expiry_start=..., expiry_end=...)

    # å›æµ‹ä¸“ç”¨: æ­¥è¿›æ—¥æœŸ
    provider.set_as_of_date(date(2024, 1, 16))
"""

import logging
from datetime import date, datetime, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any

import duckdb
import pyarrow.parquet as pq

import numpy as np

from src.data.models import (
    Fundamental,
    KlineBar,
    MacroData,
    OptionChain,
    OptionQuote,
    StockQuote,
    StockVolatility,
)
from src.data.models.option import Greeks, OptionContract, OptionType
from src.data.models.stock import KlineType
from src.data.providers.base import DataProvider

logger = logging.getLogger(__name__)


class DuckDBProvider(DataProvider):
    """DuckDB æ•°æ®æä¾›è€…

    ä»æœ¬åœ° DuckDB/Parquet è¯»å–å†å²æ•°æ®ï¼Œå®ç° DataProvider æ¥å£ã€‚

    è®¾è®¡åŸåˆ™:
    - as_of_date: å›æµ‹å½“å‰æ—¥æœŸï¼Œæ¨¡æ‹Ÿ"å½“æ—¶"åªèƒ½çœ‹åˆ°çš„æ•°æ®
    - æ‰€æœ‰æŸ¥è¯¢åªè¿”å› <= as_of_date çš„æ•°æ® (é¿å…æœªæ¥æ•°æ®æ³„éœ²)
    - ä¸å®ç›˜ Provider æ¥å£å®Œå…¨ä¸€è‡´

    Usage:
        # åˆå§‹åŒ–
        provider = DuckDBProvider(
            data_dir="/Volumes/TradingData/processed",
            as_of_date=date(2024, 1, 15),
        )

        # è·å–è‚¡ç¥¨æŠ¥ä»· (è¿”å› as_of_date å½“å¤©çš„æ”¶ç›˜æ•°æ®)
        quote = provider.get_stock_quote("AAPL")

        # è·å–æœŸæƒé“¾ (è¿”å› as_of_date å½“å¤©çš„æœŸæƒæ•°æ®)
        chain = provider.get_option_chain("AAPL")

        # æ­¥è¿›åˆ°ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
        provider.set_as_of_date(date(2024, 1, 16))
    """

    def __init__(
        self,
        data_dir: Path | str,
        as_of_date: date | None = None,
        use_duckdb: bool = False,
        db_path: str | None = None,
        auto_download_fundamental: bool = True,
        ibkr_port: int | None = None,
    ) -> None:
        """åˆå§‹åŒ– DuckDB Provider

        Args:
            data_dir: Parquet æ•°æ®ç›®å½•
            as_of_date: å›æµ‹å½“å‰æ—¥æœŸ (é»˜è®¤ä»Šå¤©)
            use_duckdb: æ˜¯å¦ä½¿ç”¨ DuckDB (False = ç›´æ¥è¯» Parquet)
            db_path: DuckDB æ•°æ®åº“è·¯å¾„ (use_duckdb=True æ—¶ä½¿ç”¨)
            auto_download_fundamental: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„åŸºæœ¬é¢æ•°æ®
            ibkr_port: IBKR TWS/Gateway ç«¯å£ (auto_download æ—¶ä½¿ç”¨)
        """
        self._data_dir = Path(data_dir)
        self._as_of_date = as_of_date or date.today()
        self._use_duckdb = use_duckdb
        self._db_path = db_path
        self._auto_download_fundamental = auto_download_fundamental
        self._ibkr_port = ibkr_port

        # DuckDB è¿æ¥ (lazy init)
        self._conn: duckdb.DuckDBPyConnection | None = None

        # ç¼“å­˜
        self._trading_days_cache: list[date] | None = None
        self._stock_quote_cache: dict[tuple[str, date], StockQuote | None] = {}
        self._option_chain_cache: dict[tuple[str, date, date | None, date | None], OptionChain | None] = {}
        self._cache_max_size = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°

        # å…¨åºåˆ—ç¼“å­˜ï¼ˆä¸éš set_as_of_date æ¸…é™¤ï¼Œå†å²æ•°æ®ä¸å¯å˜ï¼‰
        self._kline_series_cache: dict[str, list[tuple]] = {}  # symbol -> [(date, open, high, low, close, volume), ...]
        self._macro_series_cache: dict[str, list[tuple]] = {}  # indicator -> [(date, open, high, low, close), ...]
        self._stock_volatility_cache: dict[tuple[str, date], StockVolatility | None] = {}  # (symbol, date) -> result
        self._macro_blackout_cache: dict[date, tuple[bool, list]] = {}  # date -> (is_blackout, events)
        self._blackout_prefetched: bool = False  # é˜²æ­¢é‡å¤é¢„å–

        # å·²å°è¯•ä¸‹è½½çš„ symbol ç¼“å­˜ (é¿å…é‡å¤ä¸‹è½½å¤±è´¥çš„ symbol)
        self._fundamental_download_attempted: set[str] = set()

        # éªŒè¯æ•°æ®ç›®å½•
        if not self._data_dir.exists():
            logger.warning(f"Data directory does not exist: {self._data_dir}")

    def _get_conn(self) -> duckdb.DuckDBPyConnection:
        """è·å– DuckDB è¿æ¥ (lazy init)"""
        if self._conn is None:
            if self._db_path:
                self._conn = duckdb.connect(self._db_path)
            else:
                # å†…å­˜æ¨¡å¼ï¼Œä» Parquet æŸ¥è¯¢
                self._conn = duckdb.connect(":memory:")
        return self._conn

    @property
    def name(self) -> str:
        """Provider åç§°"""
        return "duckdb"

    @property
    def is_available(self) -> bool:
        """Provider æ˜¯å¦å¯ç”¨"""
        return self._data_dir.exists()

    @property
    def as_of_date(self) -> date:
        """å½“å‰å›æµ‹æ—¥æœŸ"""
        return self._as_of_date

    def set_as_of_date(self, d: date) -> None:
        """è®¾ç½®å›æµ‹æ—¥æœŸ

        Args:
            d: æ–°çš„å›æµ‹æ—¥æœŸ
        """
        if d != self._as_of_date:
            self._as_of_date = d
            # æ¸…é™¤æ—¥æœŸç›¸å…³ç¼“å­˜ (trading_days ç¼“å­˜ä¿ç•™)
            # å…¨åºåˆ—ç¼“å­˜ (_kline_series_cache, _macro_series_cache) ä¿ç•™ â€” å†å²æ•°æ®ä¸å¯å˜
            # stock_volatility_cache ä¿ç•™ â€” æŒ‰ (symbol, date) ç¼“å­˜ï¼Œä¸ä¼šæœ‰å†²çª
            self._stock_quote_cache.clear()
            self._option_chain_cache.clear()
            logger.debug(f"DuckDBProvider as_of_date set to {d}, cache cleared")

    def clear_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆåŒ…æ‹¬å…¨åºåˆ—ç¼“å­˜ï¼‰"""
        self._stock_quote_cache.clear()
        self._option_chain_cache.clear()
        self._trading_days_cache = None
        self._kline_series_cache.clear()
        self._macro_series_cache.clear()
        self._stock_volatility_cache.clear()
        self._macro_blackout_cache.clear()
        self._blackout_prefetched = False
        logger.debug("DuckDBProvider all caches cleared")

    # ========== Fundamental Auto-Download ==========

    def _has_fundamental_data(self, symbol: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¯¥ symbol çš„åŸºæœ¬é¢æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æ˜¯å¦å­˜åœ¨æ•°æ®
        """
        eps_path = self._data_dir / "fundamental_eps.parquet"
        if not eps_path.exists():
            return False

        try:
            conn = self._get_conn()
            result = conn.execute(
                f"""
                SELECT COUNT(*) FROM read_parquet('{eps_path}')
                WHERE symbol = ?
                """,
                [symbol],
            ).fetchone()
            return result[0] > 0 if result else False
        except Exception:
            return False

    def _download_fundamental_data(self, symbol: str) -> bool:
        """ä» IBKR ä¸‹è½½åŸºæœ¬é¢æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        # é¿å…é‡å¤å°è¯•å·²å¤±è´¥çš„ symbol
        if symbol in self._fundamental_download_attempted:
            return False

        self._fundamental_download_attempted.add(symbol)

        try:
            from src.backtest.data.ibkr_fundamental_downloader import IBKRFundamentalDownloader

            port = self._ibkr_port or int(__import__("os").getenv("IBKR_PORT", "7497"))

            logger.info("=" * 50)
            logger.info(f"ğŸ“¥ Auto-downloading fundamental data for {symbol}")
            logger.info(f"   Connecting to IBKR TWS/Gateway on port {port}...")
            logger.info("   (This may take 5-10 seconds)")
            logger.info("=" * 50)

            downloader = IBKRFundamentalDownloader(
                data_dir=self._data_dir,
                port=self._ibkr_port,
            )

            # è¿›åº¦å›è°ƒ
            def on_progress(sym: str, current: int, total: int):
                logger.info(f"   [{current}/{total}] Downloading {sym}...")

            results = downloader.download_and_save(
                symbols=[symbol],
                on_progress=on_progress,
                delay=0.5,
            )

            if results:
                eps_count = results.get("eps", 0)
                rev_count = results.get("revenue", 0)
                div_count = results.get("dividend", 0)
                logger.info(f"âœ… Downloaded {symbol}: EPS={eps_count}, Revenue={rev_count}, Dividend={div_count}")
                return True
            else:
                logger.warning(f"âŒ Failed to download fundamental data for {symbol}")
                logger.warning("   Please check: TWS/Gateway running? Market data subscription?")
                return False

        except ImportError as e:
            logger.warning(f"IBKRFundamentalDownloader not available: {e}")
            return False
        except Exception as e:
            logger.warning(f"âŒ Error downloading fundamental data for {symbol}: {e}")
            return False

    # å¸¸è§ ETF åˆ—è¡¨ (ETF æ²¡æœ‰ä¼ ç»Ÿçš„ EPS/Revenue æ•°æ®)
    _ETF_SYMBOLS = frozenset({
        "SPY", "QQQ", "IWM", "DIA", "VOO", "VTI", "EEM", "XLF", "XLE", "XLK",
        "GLD", "SLV", "TLT", "HYG", "LQD", "VXX", "UVXY", "SQQQ", "TQQQ",
        "ARKK", "XBI", "IBB", "SMH", "SOXX", "XOP", "OIH", "GDX", "GDXJ",
    })

    def _ensure_fundamental_data(self, symbol: str) -> bool:
        """ç¡®ä¿æœ‰è¯¥ symbol çš„åŸºæœ¬é¢æ•°æ®ï¼Œæ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æ˜¯å¦æœ‰æ•°æ®å¯ç”¨
        """
        if self._has_fundamental_data(symbol):
            return True

        if not self._auto_download_fundamental:
            return False

        # ETF æ²¡æœ‰ä¼ ç»Ÿçš„ EPS/Revenue æ•°æ®ï¼Œè·³è¿‡ä¸‹è½½
        if symbol.upper() in self._ETF_SYMBOLS:
            return False

        return self._download_fundamental_data(symbol)

    # ========== Stock Data Methods ==========

    def get_stock_quote(self, symbol: str) -> StockQuote | None:
        """è·å–è‚¡ç¥¨æŠ¥ä»· (as_of_date å½“å¤©)

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            StockQuote æˆ– None
        """
        symbol = symbol.upper()

        # æ£€æŸ¥ç¼“å­˜
        cache_key = (symbol, self._as_of_date)
        if cache_key in self._stock_quote_cache:
            return self._stock_quote_cache[cache_key]

        parquet_path = self._data_dir / "stock_daily.parquet"

        if not parquet_path.exists():
            logger.warning(f"Stock data not found: {parquet_path}")
            self._stock_quote_cache[cache_key] = None
            return None

        try:
            conn = self._get_conn()
            # ä¼˜åŒ–: åˆ—è¿‡æ»¤åœ¨ WHERE ä¹‹å‰ï¼Œå‡å°‘æ•°æ®æ‰«æ
            result = conn.execute(
                f"""
                SELECT symbol, date, open, high, low, close, volume
                FROM read_parquet('{parquet_path}')
                WHERE date = ? AND symbol = ?
                LIMIT 1
                """,
                [self._as_of_date, symbol],
            ).fetchone()

            if result is None:
                self._stock_quote_cache[cache_key] = None
                return None

            # æ˜ç¡®æŒ‡å®šåˆ—é¡ºåº: symbol, date, open, high, low, close, volume
            # æ—¥æœŸå¯èƒ½æ˜¯ date å¯¹è±¡æˆ–å­—ç¬¦ä¸²
            date_val = result[1]
            if isinstance(date_val, str):
                date_val = date.fromisoformat(date_val)
            elif isinstance(date_val, datetime):
                date_val = date_val.date()

            quote = StockQuote(
                symbol=result[0],
                timestamp=datetime.combine(date_val, datetime.min.time()),
                open=result[2],
                high=result[3],
                low=result[4],
                close=result[5],
                volume=result[6],
                source="duckdb",
            )

            # ç¼“å­˜ç»“æœ (é™åˆ¶ç¼“å­˜å¤§å°)
            if len(self._stock_quote_cache) < self._cache_max_size:
                self._stock_quote_cache[cache_key] = quote

            return quote

        except Exception as e:
            logger.error(f"Failed to get stock quote for {symbol}: {e}")
            return None

    def get_stock_quotes(self, symbols: list[str]) -> list[StockQuote]:
        """è·å–å¤šåªè‚¡ç¥¨æŠ¥ä»·

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            StockQuote åˆ—è¡¨
        """
        results = []
        for symbol in symbols:
            quote = self.get_stock_quote(symbol)
            if quote:
                results.append(quote)
        return results

    def _load_full_kline_series(self, symbol: str) -> list[tuple]:
        """åŠ è½½æŸä¸ª symbol çš„å…¨éƒ¨æ—¥çº¿æ•°æ®åˆ°å†…å­˜

        ä¸€æ¬¡æ€§ä» stock_daily.parquet è¯»å–è¯¥ symbol çš„æ‰€æœ‰è¡Œï¼Œ
        åç»­æŸ¥è¯¢ç›´æ¥åœ¨å†…å­˜ä¸­æŒ‰æ—¥æœŸè¿‡æ»¤ã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç  (å¤§å†™)

        Returns:
            [(date, open, high, low, close, volume), ...] æŒ‰æ—¥æœŸå‡åº
        """
        parquet_path = self._data_dir / "stock_daily.parquet"
        if not parquet_path.exists():
            return []

        try:
            conn = self._get_conn()
            rows = conn.execute(
                f"""
                SELECT date, open, high, low, close, volume
                FROM read_parquet('{parquet_path}')
                WHERE symbol = ?
                ORDER BY date
                """,
                [symbol],
            ).fetchall()
            logger.debug(f"Loaded full kline series for {symbol}: {len(rows)} rows")
            return rows
        except Exception as e:
            logger.error(f"Failed to load kline series for {symbol}: {e}")
            return []

    def get_history_kline(
        self,
        symbol: str,
        ktype: KlineType,
        start_date: date,
        end_date: date,
    ) -> list[KlineBar]:
        """è·å–å†å² K çº¿æ•°æ®

        æ³¨æ„: åªè¿”å› <= as_of_date çš„æ•°æ® (é¿å…æœªæ¥æ•°æ®æ³„éœ²)
        ä½¿ç”¨å…¨åºåˆ—å†…å­˜ç¼“å­˜ï¼Œé¦–æ¬¡åŠ è½½åä¸å†æŸ¥è¯¢ DuckDBã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            ktype: K çº¿ç±»å‹ (ç›®å‰åªæ”¯æŒ DAY)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            KlineBar åˆ—è¡¨
        """
        if ktype != KlineType.DAY:
            logger.warning(f"DuckDBProvider only supports daily klines, got {ktype}")
            return []

        symbol = symbol.upper()

        # é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½å…¨åºåˆ—åˆ°ç¼“å­˜
        if symbol not in self._kline_series_cache:
            self._kline_series_cache[symbol] = self._load_full_kline_series(symbol)

        # é™åˆ¶ end_date ä¸è¶…è¿‡ as_of_date
        effective_end = min(end_date, self._as_of_date)

        # åœ¨å†…å­˜ä¸­æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
        return [
            KlineBar(
                symbol=symbol,
                timestamp=datetime.combine(row[0], datetime.min.time()),
                ktype=KlineType.DAY,
                open=row[1],
                high=row[2],
                low=row[3],
                close=row[4],
                volume=row[5],
                source="duckdb",
            )
            for row in self._kline_series_cache[symbol]
            if start_date <= row[0] <= effective_end
        ]

    # ========== Option Data Methods ==========

    def get_option_chain(
        self,
        underlying: str,
        expiry_start: date | None = None,
        expiry_end: date | None = None,
        # å…¼å®¹ UnifiedDataProvider/IBKRProvider çš„å‚æ•°
        expiry_min_days: int | None = None,
        expiry_max_days: int | None = None,
        **kwargs,  # å¿½ç•¥å…¶ä»–å‚æ•°
    ) -> OptionChain | None:
        """è·å–æœŸæƒé“¾

        è¿”å› as_of_date å½“å¤©çš„æœŸæƒæ•°æ®ï¼ŒæŒ‰åˆ°æœŸæ—¥ç­›é€‰ã€‚

        Args:
            underlying: æ ‡çš„ä»£ç 
            expiry_start: åˆ°æœŸæ—¥å¼€å§‹ç­›é€‰
            expiry_end: åˆ°æœŸæ—¥ç»“æŸç­›é€‰
            expiry_min_days: æœ€å°åˆ°æœŸå¤©æ•° (ç›¸å¯¹äº as_of_date)
            expiry_max_days: æœ€å¤§åˆ°æœŸå¤©æ•° (ç›¸å¯¹äº as_of_date)
            **kwargs: å¿½ç•¥å…¶ä»–å‚æ•° (å…¼å®¹æ€§)

        Returns:
            OptionChain æˆ– None
        """
        underlying = underlying.upper()

        # å°† expiry_min_days/expiry_max_days è½¬æ¢ä¸º expiry_start/expiry_end
        if expiry_min_days is not None and expiry_start is None:
            expiry_start = self._as_of_date + timedelta(days=expiry_min_days)
        if expiry_max_days is not None and expiry_end is None:
            expiry_end = self._as_of_date + timedelta(days=expiry_max_days)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = (underlying, self._as_of_date, expiry_start, expiry_end)
        if cache_key in self._option_chain_cache:
            return self._option_chain_cache[cache_key]

        # æŸ¥æ‰¾æœŸæƒæ•°æ®
        option_dir = self._data_dir / "option_daily" / underlying
        if not option_dir.exists():
            logger.warning(f"Option data not found for {underlying}")
            self._option_chain_cache[cache_key] = None
            return None

        # ç¡®å®šè¦è¯»å–çš„ Parquet æ–‡ä»¶
        year = self._as_of_date.year
        parquet_files = []

        # ä¼˜å…ˆè¯»å–å½“å¹´çš„æ–‡ä»¶
        year_file = option_dir / f"{year}.parquet"
        if year_file.exists():
            parquet_files.append(year_file)

        # å¦‚æœå½“å¹´æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¯»å–æ‰€æœ‰æ–‡ä»¶
        if not parquet_files:
            parquet_files = list(option_dir.glob("*.parquet"))

        if not parquet_files:
            logger.warning(f"No parquet files found for {underlying}")
            self._option_chain_cache[cache_key] = None
            return None

        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = ["date = ?"]
            params: list[Any] = [self._as_of_date]

            if expiry_start:
                conditions.append("expiration >= ?")
                params.append(expiry_start)
            if expiry_end:
                conditions.append("expiration <= ?")
                params.append(expiry_end)

            where_clause = " AND ".join(conditions)

            # åˆå¹¶å¤šä¸ª Parquet æ–‡ä»¶çš„æŸ¥è¯¢
            parquet_list = ", ".join([f"'{pf}'" for pf in parquet_files])

            conn = self._get_conn()
            rows = conn.execute(
                f"""
                SELECT
                    symbol, expiration, strike, option_type, date,
                    open, high, low, close, volume, count,
                    bid, ask, delta, gamma, theta, vega, rho,
                    implied_vol, underlying_price, open_interest
                FROM read_parquet([{parquet_list}])
                WHERE {where_clause}
                ORDER BY expiration, strike, option_type
                """,
                params,
            ).fetchall()

            if not rows:
                self._option_chain_cache[cache_key] = None
                return None

            # æ„å»º OptionChain
            calls: list[OptionQuote] = []
            puts: list[OptionQuote] = []
            expiry_dates: set[date] = set()

            for row in rows:
                (
                    symbol,
                    expiration,
                    strike,
                    opt_type,
                    data_date,
                    open_price,
                    high,
                    low,
                    close,
                    volume,
                    count,
                    bid,
                    ask,
                    delta,
                    gamma,
                    theta,
                    vega,
                    rho,
                    implied_vol,
                    underlying_price,
                    open_interest,
                ) = row

                expiry_dates.add(expiration)

                # æ„å»ºæœŸæƒç¬¦å· (ç®€åŒ–æ ¼å¼)
                option_symbol = (
                    f"{underlying}{expiration.strftime('%y%m%d')}"
                    f"{'C' if opt_type == 'call' else 'P'}{int(strike * 1000):08d}"
                )

                contract = OptionContract(
                    symbol=option_symbol,
                    underlying=underlying,
                    option_type=OptionType.CALL if opt_type == "call" else OptionType.PUT,
                    strike_price=strike,
                    expiry_date=expiration,
                )

                greeks = Greeks(
                    delta=delta,
                    gamma=gamma,
                    theta=theta,
                    vega=vega,
                    rho=rho,
                )

                quote = OptionQuote(
                    contract=contract,
                    timestamp=datetime.combine(data_date, datetime.min.time()),
                    last_price=close,
                    bid=bid,
                    ask=ask,
                    volume=volume,
                    open_interest=open_interest,
                    iv=implied_vol,
                    greeks=greeks,
                    source="duckdb",
                    # OHLC ä»·æ ¼ (ç”¨äºå›æµ‹ price_mode)
                    open=open_price,
                    high=high,
                    low=low,
                    close=close,
                )

                if opt_type == "call":
                    calls.append(quote)
                else:
                    puts.append(quote)

            chain = OptionChain(
                underlying=underlying,
                timestamp=datetime.combine(self._as_of_date, datetime.min.time()),
                expiry_dates=sorted(expiry_dates),
                calls=calls,
                puts=puts,
                source="duckdb",
            )

            # ç¼“å­˜ç»“æœ
            if len(self._option_chain_cache) < self._cache_max_size:
                self._option_chain_cache[cache_key] = chain

            return chain

        except Exception as e:
            logger.error(f"Failed to get option chain for {underlying}: {e}")
            return None

    def get_option_quote(self, symbol: str) -> OptionQuote | None:
        """è·å–å•ä¸ªæœŸæƒåˆçº¦æŠ¥ä»·

        Args:
            symbol: æœŸæƒç¬¦å·

        Returns:
            OptionQuote æˆ– None
        """
        # TODO: å®ç°æœŸæƒç¬¦å·è§£æå’ŒæŸ¥è¯¢
        logger.warning("get_option_quote not fully implemented for DuckDBProvider")
        return None

    # ========== Fundamental & Macro ==========

    def get_fundamental(self, symbol: str) -> Fundamental | None:
        """è·å–åŸºæœ¬é¢æ•°æ® (å†å² point-in-time)

        ä» IBKR ä¸‹è½½çš„åŸºæœ¬é¢æ•°æ®ä¸­è¯»å–ï¼Œè¿”å› as_of_date æ—¶ç‚¹çš„åŸºæœ¬é¢ä¿¡æ¯ã€‚
        å¦‚æœæ•°æ®ä¸å­˜åœ¨ä¸” auto_download_fundamental=Trueï¼Œä¼šè‡ªåŠ¨ä» IBKR ä¸‹è½½ã€‚

        æ•°æ®æ¥æº:
        - fundamental_eps.parquet: EPS (TTM) æ•°æ®
        - fundamental_revenue.parquet: è¥æ”¶æ•°æ®
        - fundamental_dividend.parquet: è‚¡æ¯æ•°æ®

        è®¡ç®—é€»è¾‘:
        - EPS: ä½¿ç”¨æœ€è¿‘ä¸€æœŸ EPS (TTM)
        - PE: å½“æ—¥è‚¡ä»· / EPS (TTM)
        - ex_dividend_date: ä¸‹ä¸€ä¸ªé™¤æ¯æ—¥

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            Fundamental å¯¹è±¡æˆ– None
        """
        # ç¡®ä¿æœ‰è¯¥ symbol çš„åŸºæœ¬é¢æ•°æ®ï¼ˆæ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½ï¼‰
        if not self._ensure_fundamental_data(symbol):
            logger.debug(f"No fundamental data available for {symbol}")
            return None

        eps_path = self._data_dir / "fundamental_eps.parquet"
        revenue_path = self._data_dir / "fundamental_revenue.parquet"
        dividend_path = self._data_dir / "fundamental_dividend.parquet"

        # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬é¢æ•°æ®æ–‡ä»¶
        if not eps_path.exists():
            logger.debug(f"No fundamental EPS data found at {eps_path}")
            return None

        try:
            conn = self._get_conn()

            # 1. è·å–æœ€è¿‘çš„ EPS (TTM)
            # é€‰æ‹© as_of_date ä¹‹å‰æœ€è¿‘çš„ TTM EPS æ•°æ®
            eps_row = conn.execute(
                f"""
                SELECT as_of_date, eps, report_type, period
                FROM read_parquet('{eps_path}')
                WHERE symbol = ?
                  AND report_type = 'TTM'
                  AND period = '12M'
                  AND as_of_date <= ?
                ORDER BY as_of_date DESC
                LIMIT 1
                """,
                [symbol, self._as_of_date],
            ).fetchone()

            eps_value = None
            eps_date = None
            if eps_row:
                eps_date = eps_row[0]
                if isinstance(eps_date, str):
                    eps_date = date.fromisoformat(eps_date)
                elif isinstance(eps_date, datetime):
                    eps_date = eps_date.date()
                eps_value = eps_row[1]

            # 2. è·å–æœ€è¿‘çš„ Revenue (TTM)
            revenue_value = None
            if revenue_path.exists():
                rev_row = conn.execute(
                    f"""
                    SELECT as_of_date, revenue
                    FROM read_parquet('{revenue_path}')
                    WHERE symbol = ?
                      AND report_type = 'TTM'
                      AND period = '12M'
                      AND as_of_date <= ?
                    ORDER BY as_of_date DESC
                    LIMIT 1
                    """,
                    [symbol, self._as_of_date],
                ).fetchone()

                if rev_row:
                    revenue_value = rev_row[1]

            # 3. è·å–ä¸‹ä¸€ä¸ªé™¤æ¯æ—¥ (as_of_date ä¹‹åçš„ç¬¬ä¸€ä¸ª)
            ex_dividend_date = None
            if dividend_path.exists():
                div_row = conn.execute(
                    f"""
                    SELECT ex_date
                    FROM read_parquet('{dividend_path}')
                    WHERE symbol = ?
                      AND ex_date > ?
                    ORDER BY ex_date ASC
                    LIMIT 1
                    """,
                    [symbol, self._as_of_date],
                ).fetchone()

                if div_row:
                    ex_dividend_date = div_row[0]
                    if isinstance(ex_dividend_date, str):
                        ex_dividend_date = date.fromisoformat(ex_dividend_date)
                    elif isinstance(ex_dividend_date, datetime):
                        ex_dividend_date = ex_dividend_date.date()

            # 4. è·å–å½“æ—¥è‚¡ä»·è®¡ç®— PE
            pe_ratio = None
            stock_quote = self.get_stock_quote(symbol)
            if stock_quote and eps_value and eps_value != 0:
                pe_ratio = stock_quote.close / eps_value

            # 5. æ„å»º Fundamental å¯¹è±¡
            return Fundamental(
                symbol=symbol,
                date=self._as_of_date,
                eps=eps_value,
                pe_ratio=pe_ratio,
                revenue=revenue_value,
                ex_dividend_date=ex_dividend_date,
                source="duckdb",
            )

        except Exception as e:
            logger.error(f"Failed to get fundamental data for {symbol}: {e}")
            return None

    def get_historical_eps(
        self,
        symbol: str,
        start_date: date | None = None,
        end_date: date | None = None,
        report_type: str = "TTM",
    ) -> list[tuple[date, float]]:
        """è·å–å†å² EPS æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (é»˜è®¤æ— é™åˆ¶)
            end_date: ç»“æŸæ—¥æœŸ (é»˜è®¤ as_of_date)
            report_type: æŠ¥å‘Šç±»å‹ (TTM, P, R, A)

        Returns:
            [(date, eps), ...] åˆ—è¡¨
        """
        eps_path = self._data_dir / "fundamental_eps.parquet"
        if not eps_path.exists():
            return []

        effective_end = min(end_date, self._as_of_date) if end_date else self._as_of_date

        try:
            conn = self._get_conn()

            query = f"""
                SELECT as_of_date, eps
                FROM read_parquet('{eps_path}')
                WHERE symbol = ?
                  AND report_type = ?
                  AND period = '12M'
                  AND as_of_date <= ?
            """
            params = [symbol, report_type, effective_end]

            if start_date:
                query += " AND as_of_date >= ?"
                params.append(start_date)

            query += " ORDER BY as_of_date"

            rows = conn.execute(query, params).fetchall()

            results = []
            for row in rows:
                row_date = row[0]
                if isinstance(row_date, str):
                    row_date = date.fromisoformat(row_date)
                elif isinstance(row_date, datetime):
                    row_date = row_date.date()
                results.append((row_date, row[1]))

            return results

        except Exception as e:
            logger.error(f"Failed to get historical EPS for {symbol}: {e}")
            return []

    def get_dividend_dates(
        self,
        symbol: str,
        start_date: date | None = None,
        end_date: date | None = None,
    ) -> list[date]:
        """è·å–å†å²é™¤æ¯æ—¥åˆ—è¡¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            é™¤æ¯æ—¥åˆ—è¡¨
        """
        dividend_path = self._data_dir / "fundamental_dividend.parquet"
        if not dividend_path.exists():
            return []

        try:
            conn = self._get_conn()

            query = f"""
                SELECT ex_date
                FROM read_parquet('{dividend_path}')
                WHERE symbol = ?
            """
            params = [symbol]

            if start_date:
                query += " AND ex_date >= ?"
                params.append(start_date)

            if end_date:
                query += " AND ex_date <= ?"
                params.append(end_date)

            query += " ORDER BY ex_date"

            rows = conn.execute(query, params).fetchall()

            results = []
            for row in rows:
                row_date = row[0]
                if isinstance(row_date, str):
                    row_date = date.fromisoformat(row_date)
                elif isinstance(row_date, datetime):
                    row_date = row_date.date()
                results.append(row_date)

            return results

        except Exception as e:
            logger.error(f"Failed to get dividend dates for {symbol}: {e}")
            return []

    def _load_full_macro_series(self, indicator: str) -> list[tuple]:
        """åŠ è½½æŸä¸ªæŒ‡æ ‡çš„å…¨éƒ¨å®è§‚æ•°æ®åˆ°å†…å­˜

        ä¸€æ¬¡æ€§ä» macro_daily.parquet è¯»å–è¯¥ indicator çš„æ‰€æœ‰è¡Œï¼Œ
        åç»­æŸ¥è¯¢ç›´æ¥åœ¨å†…å­˜ä¸­æŒ‰æ—¥æœŸè¿‡æ»¤ã€‚

        Args:
            indicator: å®è§‚æŒ‡æ ‡ (å¦‚ ^VIX, ^TNX, SPY)

        Returns:
            [(date, open, high, low, close), ...] æŒ‰æ—¥æœŸå‡åº
        """
        parquet_path = self._data_dir / "macro_daily.parquet"
        if not parquet_path.exists():
            return []

        try:
            conn = self._get_conn()
            rows = conn.execute(
                f"""
                SELECT date, open, high, low, close
                FROM read_parquet('{parquet_path}')
                WHERE indicator = ?
                ORDER BY date
                """,
                [indicator],
            ).fetchall()
            logger.debug(f"Loaded full macro series for {indicator}: {len(rows)} rows")
            return rows
        except Exception as e:
            logger.error(f"Failed to load macro series for {indicator}: {e}")
            return []

    def get_macro_data(
        self,
        indicator: str,
        start_date: date,
        end_date: date,
    ) -> list[MacroData]:
        """è·å–å®è§‚æ•°æ® (VIX/TNX ç­‰)

        ä» macro_daily.parquet è¯»å–å†å²æ•°æ®ã€‚
        æ³¨æ„: åªè¿”å› <= as_of_date çš„æ•°æ® (é¿å…æœªæ¥æ•°æ®æ³„éœ²)
        ä½¿ç”¨å…¨åºåˆ—å†…å­˜ç¼“å­˜ï¼Œé¦–æ¬¡åŠ è½½åä¸å†æŸ¥è¯¢ DuckDBã€‚

        Args:
            indicator: å®è§‚æŒ‡æ ‡ (å¦‚ ^VIX, ^TNX, SPY)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            MacroData åˆ—è¡¨ (æŒ‰æ—¥æœŸå‡åº)
        """
        # é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½å…¨åºåˆ—åˆ°ç¼“å­˜
        if indicator not in self._macro_series_cache:
            self._macro_series_cache[indicator] = self._load_full_macro_series(indicator)

        if not self._macro_series_cache[indicator]:
            return []

        # é™åˆ¶ end_date ä¸è¶…è¿‡ as_of_date
        effective_end = min(end_date, self._as_of_date)

        # åœ¨å†…å­˜ä¸­æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
        results = []
        for row in self._macro_series_cache[indicator]:
            # row: (date, open, high, low, close)
            data_date = row[0]
            if isinstance(data_date, str):
                data_date = date.fromisoformat(data_date)
            elif isinstance(data_date, datetime):
                data_date = data_date.date()

            if start_date <= data_date <= effective_end:
                results.append(MacroData(
                    indicator=indicator,
                    date=data_date,
                    value=row[4],  # close ä½œä¸º value
                    open=row[1],
                    high=row[2],
                    low=row[3],
                    close=row[4],
                    volume=None,
                    source="duckdb",
                ))

        return results

    def get_available_macro_indicators(self) -> list[str]:
        """è·å–å¯ç”¨çš„å®è§‚æŒ‡æ ‡åˆ—è¡¨

        Returns:
            æŒ‡æ ‡åˆ—è¡¨ (å¦‚ ["^VIX", "^TNX", "SPY"])
        """
        parquet_path = self._data_dir / "macro_daily.parquet"
        if not parquet_path.exists():
            return []

        try:
            conn = self._get_conn()
            rows = conn.execute(
                f"""
                SELECT DISTINCT indicator
                FROM read_parquet('{parquet_path}')
                ORDER BY indicator
                """
            ).fetchall()
            return [row[0] for row in rows]
        except Exception as e:
            logger.error(f"Failed to get available macro indicators: {e}")
            return []

    # ========== Backtest-Specific Methods ==========

    def get_trading_days(
        self,
        start_date: date,
        end_date: date,
        symbol: str | None = None,
    ) -> list[date]:
        """è·å–äº¤æ˜“æ—¥åˆ—è¡¨

        ä»å†å²æ•°æ®ä¸­æå–æœ‰æ•°æ®çš„æ—¥æœŸä½œä¸ºäº¤æ˜“æ—¥ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            symbol: æ ‡çš„ä»£ç  (å¯é€‰ï¼Œç”¨äºè¿‡æ»¤)

        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨ (å‡åº)
        """
        parquet_path = self._data_dir / "stock_daily.parquet"

        if not parquet_path.exists():
            # å°è¯•ä»æœŸæƒæ•°æ®è·å–
            option_base = self._data_dir / "option_daily"
            if not option_base.exists():
                return []

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æ ‡çš„
            symbol_dirs = list(option_base.iterdir())
            if not symbol_dirs:
                return []

            parquet_files = list(symbol_dirs[0].glob("*.parquet"))
            if not parquet_files:
                return []

            parquet_path = parquet_files[0]
            date_col = "date"
        else:
            date_col = "date"

        try:
            conn = self._get_conn()

            if symbol:
                rows = conn.execute(
                    f"""
                    SELECT DISTINCT {date_col}
                    FROM read_parquet('{parquet_path}')
                    WHERE symbol = ?
                      AND {date_col} >= ?
                      AND {date_col} <= ?
                    ORDER BY {date_col}
                    """,
                    [symbol.upper(), start_date, end_date],
                ).fetchall()
            else:
                rows = conn.execute(
                    f"""
                    SELECT DISTINCT {date_col}
                    FROM read_parquet('{parquet_path}')
                    WHERE {date_col} >= ?
                      AND {date_col} <= ?
                    ORDER BY {date_col}
                    """,
                    [start_date, end_date],
                ).fetchall()

            return [row[0] for row in rows]

        except Exception as e:
            logger.error(f"Failed to get trading days: {e}")
            return []

    def get_next_trading_day(self, d: date | None = None) -> date | None:
        """è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥

        Args:
            d: åŸºå‡†æ—¥æœŸ (é»˜è®¤ as_of_date)

        Returns:
            ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥æˆ– None
        """
        base = d or self._as_of_date

        # è·å–äº¤æ˜“æ—¥åˆ—è¡¨ (ç¼“å­˜)
        if self._trading_days_cache is None:
            # è·å–æœªæ¥ä¸€å¹´çš„äº¤æ˜“æ—¥
            self._trading_days_cache = self.get_trading_days(
                base, base + timedelta(days=365)
            )

        for td in self._trading_days_cache:
            if td > base:
                return td

        return None

    def get_available_symbols(self) -> list[str]:
        """è·å–å¯ç”¨çš„æ ‡çš„åˆ—è¡¨

        Returns:
            æ ‡çš„ä»£ç åˆ—è¡¨
        """
        option_dir = self._data_dir / "option_daily"
        if option_dir.exists():
            return sorted([d.name for d in option_dir.iterdir() if d.is_dir()])
        return []

    def close(self) -> None:
        """å…³é—­è¿æ¥"""
        if self._conn:
            self._conn.close()
            self._conn = None

    def create_optimized_db(
        self,
        db_path: str | Path,
        symbols: list[str] | None = None,
    ) -> Path:
        """åˆ›å»ºä¼˜åŒ–çš„ DuckDB æ•°æ®åº“

        å°† Parquet æ•°æ®å¯¼å…¥ DuckDB å¹¶åˆ›å»ºç´¢å¼•ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½ã€‚
        é€‚åˆéœ€è¦åå¤æŸ¥è¯¢çš„å¤§è§„æ¨¡å›æµ‹åœºæ™¯ã€‚

        Args:
            db_path: DuckDB æ•°æ®åº“è·¯å¾„
            symbols: è¦å¯¼å…¥çš„æ ‡çš„åˆ—è¡¨ (None è¡¨ç¤ºå…¨éƒ¨)

        Returns:
            æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        from src.backtest.data.schema import (
            StockDailySchema,
            OptionDailySchema,
            init_duckdb_schema,
            load_parquet_to_duckdb,
        )

        db_path = Path(db_path)
        db_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ–°æ•°æ®åº“
        conn = duckdb.connect(str(db_path))

        try:
            # åˆå§‹åŒ– schema å’Œç´¢å¼•
            init_duckdb_schema(conn)

            # åŠ è½½æ•°æ®
            load_parquet_to_duckdb(conn, self._data_dir, symbols)

            # åˆ›å»ºé¢å¤–çš„å¤åˆç´¢å¼•ä»¥ä¼˜åŒ–å¸¸è§æŸ¥è¯¢
            # æœŸæƒé“¾æŸ¥è¯¢: WHERE date = ? AND symbol = ? AND expiration >= ? AND expiration <= ?
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_option_daily_query
                ON option_daily(date, symbol, expiration)
            """)

            # è‚¡ç¥¨æŸ¥è¯¢: WHERE date = ? AND symbol = ?
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_stock_daily_query
                ON stock_daily(date, symbol)
            """)

            # åˆ†æè¡¨ä»¥æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            conn.execute("ANALYZE stock_daily")
            conn.execute("ANALYZE option_daily")

            logger.info(f"Created optimized DuckDB at {db_path}")

        finally:
            conn.close()

        return db_path

    def use_optimized_db(self, db_path: str | Path) -> None:
        """åˆ‡æ¢åˆ°ä½¿ç”¨ä¼˜åŒ–çš„ DuckDB æ•°æ®åº“

        Args:
            db_path: DuckDB æ•°æ®åº“è·¯å¾„
        """
        self.close()
        self._db_path = str(db_path)
        self._use_duckdb = True
        self._conn = None  # Will be lazily initialized
        self.clear_cache()
        logger.info(f"Switched to optimized DuckDB: {db_path}")

    # ========== Screening Support Methods ==========

    def get_option_quotes_batch(
        self,
        contracts: list[OptionContract],
        min_volume: int | None = None,
        fetch_margin: bool = False,  # å…¼å®¹ UnifiedDataProvider (å›æµ‹å¿½ç•¥)
        underlying_price: float | None = None,  # å…¼å®¹ UnifiedDataProvider
        **kwargs,  # å¿½ç•¥å…¶ä»–å‚æ•°
    ) -> list[OptionQuote]:
        """è·å–æ‰¹é‡æœŸæƒåˆçº¦æŠ¥ä»·

        ä» option_daily Parquet æ•°æ®è·å–æŒ‡å®šåˆçº¦çš„æŠ¥ä»·ä¿¡æ¯ã€‚

        Args:
            contracts: è¦æŸ¥è¯¢çš„æœŸæƒåˆçº¦åˆ—è¡¨
            min_volume: å¯é€‰çš„æœ€å°æˆäº¤é‡è¿‡æ»¤
            fetch_margin: æ˜¯å¦è·å–ä¿è¯é‡‘ (å›æµ‹æ¨¡å¼å¿½ç•¥)
            underlying_price: æ ‡çš„ä»·æ ¼ (å›æµ‹æ¨¡å¼å¿½ç•¥)
            **kwargs: å¿½ç•¥å…¶ä»–å‚æ•° (å…¼å®¹æ€§)

        Returns:
            OptionQuote åˆ—è¡¨
        """
        if not contracts:
            return []

        results: list[OptionQuote] = []

        # æŒ‰ underlying åˆ†ç»„æŸ¥è¯¢ï¼Œæé«˜æ•ˆç‡
        contracts_by_underlying: dict[str, list[OptionContract]] = {}
        for contract in contracts:
            underlying = contract.underlying.upper()
            if underlying not in contracts_by_underlying:
                contracts_by_underlying[underlying] = []
            contracts_by_underlying[underlying].append(contract)

        for underlying, underlying_contracts in contracts_by_underlying.items():
            # æŸ¥æ‰¾æœŸæƒæ•°æ®ç›®å½•
            option_dir = self._data_dir / "option_daily" / underlying
            if not option_dir.exists():
                logger.debug(f"Option data not found for {underlying}")
                continue

            # ç¡®å®š Parquet æ–‡ä»¶
            year = self._as_of_date.year
            parquet_file = option_dir / f"{year}.parquet"
            if not parquet_file.exists():
                # å°è¯•å…¶ä»–å¹´ä»½æ–‡ä»¶
                parquet_files = list(option_dir.glob("*.parquet"))
                if not parquet_files:
                    continue
                parquet_file = parquet_files[0]

            try:
                conn = self._get_conn()

                for contract in underlying_contracts:
                    # è½¬æ¢ option_type
                    opt_type_str = "call" if contract.option_type == OptionType.CALL else "put"

                    # æŸ¥è¯¢ç‰¹å®šåˆçº¦
                    row = conn.execute(
                        f"""
                        SELECT
                            symbol, expiration, strike, option_type, date,
                            open, high, low, close, volume, count,
                            bid, ask, delta, gamma, theta, vega, rho,
                            implied_vol, underlying_price, open_interest
                        FROM read_parquet('{parquet_file}')
                        WHERE date = ?
                          AND expiration = ?
                          AND strike = ?
                          AND option_type = ?
                        LIMIT 1
                        """,
                        [
                            self._as_of_date,
                            contract.expiry_date,
                            contract.strike_price,
                            opt_type_str,
                        ],
                    ).fetchone()

                    if row is None:
                        continue

                    # æ£€æŸ¥æœ€å°æˆäº¤é‡
                    volume = row[9] or 0
                    if min_volume is not None and volume < min_volume:
                        continue

                    # æ„å»º OptionQuote
                    (
                        symbol,
                        expiration,
                        strike,
                        opt_type,
                        data_date,
                        open_price,
                        high,
                        low,
                        close,
                        volume,
                        count,
                        bid,
                        ask,
                        delta,
                        gamma,
                        theta,
                        vega,
                        rho,
                        implied_vol,
                        underlying_price,
                        open_interest,
                    ) = row

                    greeks = Greeks(
                        delta=delta,
                        gamma=gamma,
                        theta=theta,
                        vega=vega,
                        rho=rho,
                    )

                    quote = OptionQuote(
                        contract=contract,
                        timestamp=datetime.combine(data_date, datetime.min.time()),
                        last_price=close,
                        bid=bid,
                        ask=ask,
                        volume=volume,
                        open_interest=open_interest or 0,
                        iv=implied_vol,
                        greeks=greeks,
                        source="duckdb",
                        # OHLC ä»·æ ¼ (ç”¨äºå›æµ‹ price_mode)
                        open=open_price,
                        high=high,
                        low=low,
                        close=close,
                    )
                    results.append(quote)

            except Exception as e:
                logger.error(f"Failed to get option quotes for {underlying}: {e}")

        return results

    def _prefetch_economic_calendar(
        self,
        blackout_days: int,
        blackout_events: list[str],
    ) -> None:
        """ä»æœ¬åœ° economic_calendar.json åŠ è½½ç»æµæ—¥å†ï¼Œé¢„è®¡ç®—æ‰€æœ‰äº¤æ˜“æ—¥çš„é»‘åå•çŠ¶æ€

        é¦–æ¬¡è°ƒç”¨ check_macro_blackout æ—¶è§¦å‘ã€‚æ•°æ®æ¥æºäºæ•°æ®ä¸‹è½½é˜¶æ®µ
        (scripts/download_backtest_data.py) é¢„ç”Ÿæˆçš„ JSON æ–‡ä»¶ï¼Œä¸éœ€è¦åœ¨çº¿ APIã€‚
        """
        from datetime import timedelta

        try:
            import json

            from src.data.models.event import EconomicEventType, EventCalendar

            # 1. ä»æœ¬åœ° JSON åŠ è½½ç»æµæ—¥å†
            cal_path = self._data_dir / "economic_calendar.json"
            if not cal_path.exists():
                logger.warning(f"Economic calendar not found: {cal_path}, blackout check disabled")
                return

            with open(cal_path, "r", encoding="utf-8") as f:
                cal_data = json.load(f)

            calendar = EventCalendar.from_dict(cal_data)
            logger.info(
                f"Economic calendar loaded from {cal_path.name}: "
                f"{len(calendar.events)} events ({calendar.start_date} ~ {calendar.end_date})"
            )

            # 2. æŒ‰ event_types è¿‡æ»¤ (e.g. ["FOMC", "CPI", "NFP"])
            type_map = {t.name: t for t in EconomicEventType}
            filter_types = [type_map[t] for t in blackout_events if t in type_map]
            if filter_types:
                calendar = calendar.filter_by_type(filter_types)

            all_events = calendar.events
            if not all_events:
                logger.info("No matching economic events found for blackout check")
                return

            # 3. è·å–äº¤æ˜“æ—¥åˆ—è¡¨
            if self._trading_days_cache:
                trading_days = self._trading_days_cache
            else:
                trading_days = self.get_trading_days(
                    calendar.start_date, calendar.end_date
                )

            # 4. ä¸ºæ¯ä¸ªäº¤æ˜“æ—¥é¢„è®¡ç®—é»‘åå•çŠ¶æ€
            for day in trading_days:
                day_end = day + timedelta(days=blackout_days)
                causing = [e for e in all_events if day <= e.event_date <= day_end]
                self._macro_blackout_cache[day] = (len(causing) > 0, causing)

            logger.info(
                f"Economic calendar prefetched: {len(trading_days)} days cached, "
                f"{len(all_events)} events matched ({', '.join(blackout_events)})"
            )

        except Exception as e:
            logger.warning(f"Failed to prefetch economic calendar: {e}")

    def check_macro_blackout(
        self,
        target_date: date | None = None,
        blackout_days: int = 2,
        blackout_events: list[str] | None = None,
    ) -> tuple[bool, list]:
        """æ£€æŸ¥æ˜¯å¦å¤„äºå®è§‚äº‹ä»¶é»‘åå•æœŸ

        ä½¿ç”¨ EconomicCalendarProvider æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å¤„äºé‡å¤§å®è§‚äº‹ä»¶
        (FOMC/CPI/NFP) çš„é»‘åå•æœŸã€‚é¦–æ¬¡è°ƒç”¨æ—¶ä¸€æ¬¡æ€§é¢„å–æ•´ä¸ªå›æµ‹æœŸé—´çš„æ—¥å†ã€‚

        Args:
            target_date: è¦æ£€æŸ¥çš„æ—¥æœŸ (é»˜è®¤ as_of_date)
            blackout_days: äº‹ä»¶å‰å‡ å¤©å¼€å§‹é»‘åå•æœŸ
            blackout_events: è¦æ£€æŸ¥çš„äº‹ä»¶ç±»å‹åˆ—è¡¨ (é»˜è®¤ ["FOMC", "CPI", "NFP"])

        Returns:
            (æ˜¯å¦å¤„äºé»‘åå•æœŸ, å³å°†åˆ°æ¥çš„äº‹ä»¶åˆ—è¡¨)
        """
        if target_date is None:
            target_date = self._as_of_date

        if blackout_events is None:
            blackout_events = ["FOMC", "CPI", "NFP"]

        # é¦–æ¬¡è°ƒç”¨æ—¶ä»æœ¬åœ° JSON é¢„å–æ•´ä¸ªå›æµ‹æœŸé—´çš„ç»æµæ—¥å†
        if not self._blackout_prefetched:
            self._blackout_prefetched = True
            self._prefetch_economic_calendar(blackout_days, blackout_events)

        # ä»ç¼“å­˜è¿”å›
        if target_date in self._macro_blackout_cache:
            return self._macro_blackout_cache[target_date]

        # ç¼“å­˜æœªå‘½ä¸­ï¼ˆæ—¥æœŸä¸åœ¨äº¤æ˜“æ—¥åˆ—è¡¨ä¸­ï¼‰â€” fall-open
        return False, []

    def get_stock_beta(self, symbol: str, as_of_date: date | None = None) -> float | None:
        """è·å–è‚¡ç¥¨ Beta å€¼

        ä¼˜å…ˆä» stock_beta_daily.parquet è¯»å–åŠ¨æ€æ»šåŠ¨ Betaï¼Œ
        å¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° stock_beta.parquet é™æ€ Betaã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            as_of_date: æŸ¥è¯¢æ—¥æœŸ (å¦‚æœä¸º Noneï¼Œè¿”å›æœ€æ–°å€¼)

        Returns:
            Beta å€¼æˆ– None
        """
        conn = self._get_conn()

        # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€æ»šåŠ¨ Beta (stock_beta_daily.parquet)
        rolling_beta_path = self._data_dir / "stock_beta_daily.parquet"
        if rolling_beta_path.exists():
            try:
                if as_of_date:
                    # æŸ¥è¯¢æŒ‡å®šæ—¥æœŸæˆ–ä¹‹å‰æœ€è¿‘çš„ Beta
                    result = conn.execute(
                        f"""
                        SELECT beta FROM read_parquet('{rolling_beta_path}')
                        WHERE symbol = ? AND date <= ?
                        ORDER BY date DESC LIMIT 1
                        """,
                        [symbol.upper(), as_of_date],
                    ).fetchone()
                else:
                    # æŸ¥è¯¢æœ€æ–° Beta
                    result = conn.execute(
                        f"""
                        SELECT beta FROM read_parquet('{rolling_beta_path}')
                        WHERE symbol = ?
                        ORDER BY date DESC LIMIT 1
                        """,
                        [symbol.upper()],
                    ).fetchone()

                if result:
                    return float(result[0])
            except Exception as e:
                logger.warning(f"Failed to get rolling beta for {symbol}: {e}")

        # å›é€€åˆ°é™æ€ Beta (stock_beta.parquet)
        static_beta_path = self._data_dir / "stock_beta.parquet"
        if static_beta_path.exists():
            try:
                result = conn.execute(
                    f"SELECT beta FROM read_parquet('{static_beta_path}') WHERE symbol = ?",
                    [symbol.upper()],
                ).fetchone()
                if result:
                    return float(result[0])
            except Exception as e:
                logger.warning(f"Failed to get static beta for {symbol}: {e}")

        logger.debug(f"Beta data not found for {symbol}")
        return None

    def get_stock_volatility(self, symbol: str) -> StockVolatility | None:
        """è·å–è‚¡ç¥¨æ³¢åŠ¨ç‡æŒ‡æ ‡

        è®¡ç®—è‚¡ç¥¨çš„ IV å’Œ HV:
        - HV: 20 æ—¥å†å²æ³¢åŠ¨ç‡ (ä» stock_daily è®¡ç®—)
        - IV: ATM æœŸæƒçš„å¹³å‡éšå«æ³¢åŠ¨ç‡ (ä» option_daily è·å–)

        ç»“æœæŒ‰ (symbol, as_of_date) ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®— IV Rank ç­‰æ˜‚è´µæ“ä½œã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            StockVolatility å¯¹è±¡æˆ– None
        """
        symbol = symbol.upper()

        # æ£€æŸ¥ç¼“å­˜
        cache_key = (symbol, self._as_of_date)
        if cache_key in self._stock_volatility_cache:
            return self._stock_volatility_cache[cache_key]

        # 1. è®¡ç®— 60 æ—¥å†å²æ³¢åŠ¨ç‡
        hv = self._calculate_historical_volatility(symbol, lookback_days=60)
        if hv is None:
            logger.debug(f"Cannot calculate HV for {symbol}, insufficient data")
            self._stock_volatility_cache[cache_key] = None
            return None

        # 2. è·å– ATM æœŸæƒçš„ IV
        iv = self._get_atm_implied_volatility(symbol)

        # 3. è®¡ç®— IV Rank å’Œ IV Percentile
        iv_rank = None
        iv_percentile = None
        if iv is not None:
            iv_rank, iv_percentile = self._calculate_iv_rank(symbol, iv)

        result = StockVolatility(
            symbol=symbol,
            timestamp=datetime.combine(self._as_of_date, datetime.min.time()),
            iv=iv,
            hv=hv,
            iv_rank=iv_rank,
            iv_percentile=iv_percentile,
            pcr=None,  # éœ€è¦æˆäº¤é‡æ•°æ®è®¡ç®—
            source="duckdb",
        )
        self._stock_volatility_cache[cache_key] = result
        return result

    def _calculate_historical_volatility(
        self,
        symbol: str,
        lookback_days: int = 20,
    ) -> float | None:
        """è®¡ç®—å†å²æ³¢åŠ¨ç‡ (å¹´åŒ–)

        ä½¿ç”¨æœ€è¿‘ N å¤©çš„æ”¶ç›˜ä»·è®¡ç®—æ—¥æ”¶ç›Šç‡æ ‡å‡†å·®ï¼Œå†å¹´åŒ–ã€‚
        åˆ©ç”¨ kline å…¨åºåˆ—ç¼“å­˜ï¼Œé¿å…é‡å¤æŸ¥è¯¢ DuckDBã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            lookback_days: å›æº¯å¤©æ•° (é»˜è®¤ 20)

        Returns:
            å¹´åŒ–å†å²æ³¢åŠ¨ç‡ (å°æ•°å½¢å¼) æˆ– None
        """
        # ç¡®ä¿ kline ç¼“å­˜å·²åŠ è½½
        if symbol not in self._kline_series_cache:
            self._kline_series_cache[symbol] = self._load_full_kline_series(symbol)

        series = self._kline_series_cache[symbol]
        if not series:
            return None

        try:
            # è¿‡æ»¤ <= as_of_date çš„æ•°æ® (series å·²æŒ‰æ—¥æœŸå‡åº)
            eligible = [row for row in series if row[0] <= self._as_of_date]

            if len(eligible) < lookback_days + 1:
                logger.debug(f"Not enough data for HV calculation: got {len(eligible)}, need {lookback_days + 1}")
                return None

            # å–æœ€è¿‘ N+1 å¤©çš„æ”¶ç›˜ä»· (close åœ¨ index 4)
            closes = [row[4] for row in eligible[-(lookback_days + 1):]]

            # è®¡ç®—æ—¥æ”¶ç›Šç‡
            returns = np.diff(np.log(closes))

            # å¹´åŒ–æ³¢åŠ¨ç‡ (å‡è®¾ 252 ä¸ªäº¤æ˜“æ—¥)
            hv = float(np.std(returns) * np.sqrt(252))
            return hv

        except Exception as e:
            logger.error(f"Failed to calculate HV for {symbol}: {e}")
            return None

    def _get_atm_implied_volatility(self, symbol: str) -> float | None:
        """è·å– ATM æœŸæƒçš„å¹³å‡éšå«æ³¢åŠ¨ç‡

        æ‰¾åˆ°æ¥è¿‘å½“å‰è‚¡ä»·çš„æœŸæƒ (moneyness 0.95-1.05)ï¼Œè®¡ç®—å¹³å‡ IVã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            å¹³å‡ IV (å°æ•°å½¢å¼) æˆ– None
        """
        # è·å–å½“å‰è‚¡ä»·
        stock_quote = self.get_stock_quote(symbol)
        if stock_quote is None:
            return None

        underlying_price = stock_quote.close

        # æŸ¥æ‰¾æœŸæƒæ•°æ®
        option_dir = self._data_dir / "option_daily" / symbol
        if not option_dir.exists():
            return None

        # ç¡®å®š Parquet æ–‡ä»¶
        year = self._as_of_date.year
        parquet_file = option_dir / f"{year}.parquet"
        if not parquet_file.exists():
            parquet_files = list(option_dir.glob("*.parquet"))
            if not parquet_files:
                return None
            parquet_file = parquet_files[0]

        try:
            conn = self._get_conn()

            # ATM èŒƒå›´: strike åœ¨ underlying_price çš„ 95%-105% ä¹‹é—´
            strike_low = underlying_price * 0.95
            strike_high = underlying_price * 1.05

            rows = conn.execute(
                f"""
                SELECT implied_vol
                FROM read_parquet('{parquet_file}')
                WHERE date = ?
                  AND strike >= ?
                  AND strike <= ?
                  AND implied_vol > 0
                  AND implied_vol < 5
                """,
                [self._as_of_date, strike_low, strike_high],
            ).fetchall()

            if not rows:
                return None

            # è®¡ç®—å¹³å‡ IV
            ivs = [row[0] for row in rows if row[0] is not None]
            if not ivs:
                return None

            return float(np.mean(ivs))

        except Exception as e:
            logger.error(f"Failed to get ATM IV for {symbol}: {e}")
            return None

    def _calculate_iv_rank(
        self,
        symbol: str,
        current_iv: float,
        lookback_days: int = 252,
    ) -> tuple[float | None, float | None]:
        """è®¡ç®— IV Rank å’Œ IV Percentile

        åŸºäºæ ‡çš„è‚¡ç¥¨çš„æ¯æ—¥ ATM IV å†å²è®¡ç®—:
        - IV Rank = (current - min) / (max - min) * 100
        - IV Percentile = è¿‡å» N å¤©ä¸­ IV < current çš„å¤©æ•°å æ¯” * 100

        ä½¿ç”¨ underlying_price åˆ—è®©æ¯å¤©çš„ ATM èŒƒå›´åŸºäºå½“å¤©å®é™…è‚¡ä»·ã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            current_iv: å½“å‰ IV (å°æ•°å½¢å¼)
            lookback_days: å›æº¯å¤©æ•° (é»˜è®¤ 252)

        Returns:
            (iv_rank, iv_percentile) å…ƒç»„ï¼Œä¸å¯ç”¨æ—¶è¿”å› (None, None)
        """
        option_dir = self._data_dir / "option_daily" / symbol
        if not option_dir.exists():
            return None, None

        parquet_files = sorted(option_dir.glob("*.parquet"))
        if not parquet_files:
            return None, None

        try:
            conn = self._get_conn()
            from datetime import timedelta

            lookback_start = self._as_of_date - timedelta(days=int(lookback_days * 1.5))

            # ä»æ‰€æœ‰ç›¸å…³ parquet æ–‡ä»¶æŸ¥è¯¢ï¼Œç”¨ underlying_price åŠ¨æ€è®¡ç®—æ¯å¤©çš„ ATM èŒƒå›´
            union_parts = []
            for pf in parquet_files:
                union_parts.append(
                    f"SELECT date, implied_vol FROM read_parquet('{pf}') "
                    f"WHERE date >= '{lookback_start}' AND date < '{self._as_of_date}' "
                    f"AND strike >= underlying_price * 0.95 "
                    f"AND strike <= underlying_price * 1.05 "
                    f"AND implied_vol > 0 AND implied_vol < 5"
                )

            if not union_parts:
                return None, None

            union_sql = " UNION ALL ".join(union_parts)
            rows = conn.execute(
                f"SELECT date, MEDIAN(implied_vol) as daily_iv "
                f"FROM ({union_sql}) "
                f"GROUP BY date ORDER BY date"
            ).fetchall()

            if len(rows) < 20:
                logger.debug(
                    f"Not enough IV history for {symbol}: {len(rows)} days, need >= 20"
                )
                return None, None

            historical_ivs = [row[1] for row in rows if row[1] is not None]
            if len(historical_ivs) < 20:
                return None, None

            iv_min = min(historical_ivs)
            iv_max = max(historical_ivs)

            # IV Rank
            iv_rank = None
            if iv_max > iv_min:
                iv_rank = (current_iv - iv_min) / (iv_max - iv_min) * 100
                iv_rank = max(0.0, min(100.0, iv_rank))

            # IV Percentile
            lower_count = sum(1 for h in historical_ivs if h < current_iv)
            iv_percentile = lower_count / len(historical_ivs) * 100

            return iv_rank, iv_percentile

        except Exception as e:
            logger.debug(f"Failed to calculate IV rank for {symbol}: {e}")
            return None, None
