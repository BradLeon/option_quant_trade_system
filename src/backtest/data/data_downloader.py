"""
Data Downloader - æ‰¹é‡æ•°æ®ä¸‹è½½å·¥å…·

ä» ThetaData ä¸‹è½½å†å²æ•°æ®å¹¶ä¿å­˜ä¸º Parquet æ–‡ä»¶ã€‚

æ”¯æŒåŠŸèƒ½:
- å¤šæ ‡çš„æ‰¹é‡ä¸‹è½½
- æ—¥æœŸèŒƒå›´åˆ†å—ä¸‹è½½
- è¿›åº¦è¿½è¸ªå’Œæ–­ç‚¹ç»­ä¼ 
- è‡ªåŠ¨ rate limit å¤„ç†

Usage:
    downloader = DataDownloader(
        data_dir="/Volumes/TradingData/processed",
        client=ThetaDataClient(),
    )

    # ä¸‹è½½è‚¡ç¥¨æ•°æ®
    downloader.download_stocks(["AAPL", "MSFT"], date(2015, 1, 1), date(2024, 12, 31))

    # ä¸‹è½½æœŸæƒæ•°æ®
    downloader.download_options(["AAPL"], date(2020, 1, 1), date(2024, 12, 31))
"""

import json
import logging
from dataclasses import asdict, dataclass
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Callable

import pyarrow as pa
import pyarrow.parquet as pq

from src.backtest.data.schema import (
    OptionDailySchema,
    StockDailySchema,
    get_parquet_path,
)
from src.backtest.data.thetadata_client import (
    OptionEODGreeks,
    StockEOD,
    ThetaDataClient,
)

logger = logging.getLogger(__name__)


@dataclass
class DownloadProgress:
    """ä¸‹è½½è¿›åº¦è¿½è¸ª"""

    symbol: str
    data_type: str  # "stock" or "option"
    start_date: date
    end_date: date
    last_completed_date: date | None = None
    total_records: int = 0
    status: str = "pending"  # pending, in_progress, completed, failed
    error_message: str | None = None

    def to_dict(self) -> dict:
        return {
            "symbol": self.symbol,
            "data_type": self.data_type,
            "start_date": self.start_date.isoformat(),
            "end_date": self.end_date.isoformat(),
            "last_completed_date": self.last_completed_date.isoformat()
            if self.last_completed_date
            else None,
            "total_records": self.total_records,
            "status": self.status,
            "error_message": self.error_message,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "DownloadProgress":
        return cls(
            symbol=data["symbol"],
            data_type=data["data_type"],
            start_date=date.fromisoformat(data["start_date"]),
            end_date=date.fromisoformat(data["end_date"]),
            last_completed_date=date.fromisoformat(data["last_completed_date"])
            if data.get("last_completed_date")
            else None,
            total_records=data.get("total_records", 0),
            status=data.get("status", "pending"),
            error_message=data.get("error_message"),
        )


class DataDownloader:
    """æ‰¹é‡æ•°æ®ä¸‹è½½å™¨

    ä» ThetaData ä¸‹è½½å†å²æ•°æ®å¹¶ä¿å­˜ä¸º Parquet æ–‡ä»¶ã€‚

    Features:
    - å¤šæ ‡çš„æ‰¹é‡ä¸‹è½½
    - æ—¥æœŸèŒƒå›´åˆ†å— (é¿å…å•æ¬¡è¯·æ±‚æ•°æ®é‡è¿‡å¤§)
    - æ–­ç‚¹ç»­ä¼  (è®°å½•è¿›åº¦)
    - è‡ªåŠ¨ rate limit å¤„ç†
    """

    def __init__(
        self,
        data_dir: Path | str,
        client: ThetaDataClient | None = None,
    ) -> None:
        """åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            client: ThetaData å®¢æˆ·ç«¯ (å¯é€‰)
        """
        self._data_dir = Path(data_dir)
        self._data_dir.mkdir(parents=True, exist_ok=True)

        self._client = client or ThetaDataClient()
        self._progress_file = self._data_dir / ".download_progress.json"
        self._progress: dict[str, DownloadProgress] = {}

        self._load_progress()

    def _load_progress(self) -> None:
        """åŠ è½½ä¸‹è½½è¿›åº¦"""
        if self._progress_file.exists():
            try:
                with open(self._progress_file) as f:
                    data = json.load(f)
                self._progress = {
                    k: DownloadProgress.from_dict(v) for k, v in data.items()
                }
            except Exception as e:
                logger.warning(f"Failed to load progress file: {e}")
                self._progress = {}

    def _save_progress(self) -> None:
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        try:
            data = {k: v.to_dict() for k, v in self._progress.items()}
            with open(self._progress_file, "w") as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save progress file: {e}")

    def _get_progress_key(self, symbol: str, data_type: str) -> str:
        """ç”Ÿæˆè¿›åº¦ key"""
        return f"{data_type}:{symbol}"

    # ========== Stock Data Download ==========

    def download_stocks(
        self,
        symbols: list[str],
        start_date: date,
        end_date: date,
        on_progress: Callable[[str, int, int], None] | None = None,
    ) -> dict[str, int]:
        """ä¸‹è½½è‚¡ç¥¨å†å²æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            on_progress: è¿›åº¦å›è°ƒ (symbol, downloaded, total)

        Returns:
            {symbol: record_count} ä¸‹è½½è®°å½•æ•°
        """
        results = {}
        total = len(symbols)

        for i, symbol in enumerate(symbols):
            if on_progress:
                on_progress(symbol, i, total)

            try:
                count = self._download_stock(symbol, start_date, end_date)
                results[symbol] = count
                logger.info(f"Downloaded {count} records for {symbol}")
            except Exception as e:
                logger.error(f"Failed to download {symbol}: {e}")
                results[symbol] = 0

        # æ›´æ–°æ•°æ®ç›®å½•
        self.update_catalog()

        return results

    def _download_stock(
        self,
        symbol: str,
        start_date: date,
        end_date: date,
    ) -> int:
        """ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä¸‹è½½çš„è®°å½•æ•°
        """
        progress_key = self._get_progress_key(symbol, "stock")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„è¿›åº¦
        progress = self._progress.get(progress_key)
        if progress and progress.status == "completed":
            if progress.start_date <= start_date and progress.end_date >= end_date:
                logger.info(f"Stock {symbol} already downloaded")
                return progress.total_records

        # åˆå§‹åŒ–è¿›åº¦
        progress = DownloadProgress(
            symbol=symbol,
            data_type="stock",
            start_date=start_date,
            end_date=end_date,
            status="in_progress",
        )
        self._progress[progress_key] = progress
        self._save_progress()

        # ä¸‹è½½æ•°æ®
        all_records: list[StockEOD] = []

        try:
            # è‚¡ç¥¨æ•°æ®é‡è¾ƒå°ï¼Œå¯ä»¥ä¸€æ¬¡æ€§è¯·æ±‚
            records = self._client.get_stock_eod(symbol, start_date, end_date)
            all_records.extend(records)

            # ä¿å­˜ä¸º Parquet
            if all_records:
                self._save_stock_parquet(symbol, all_records)

            # æ›´æ–°è¿›åº¦
            progress.total_records = len(all_records)
            progress.last_completed_date = end_date
            progress.status = "completed"
            self._save_progress()

            return len(all_records)

        except Exception as e:
            progress.status = "failed"
            progress.error_message = str(e)
            self._save_progress()
            raise

    def _save_stock_parquet(self, symbol: str, records: list[StockEOD]) -> None:
        """ä¿å­˜è‚¡ç¥¨æ•°æ®ä¸º Parquet

        è¿½åŠ æ¨¡å¼ï¼šå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®å¹¶åˆå¹¶å»é‡ã€‚
        """
        parquet_path = get_parquet_path(self._data_dir, "stock")

        # è½¬æ¢ä¸º PyArrow Table
        data = {
            "symbol": [r.symbol for r in records],
            "date": [r.date for r in records],
            "open": [r.open for r in records],
            "high": [r.high for r in records],
            "low": [r.low for r in records],
            "close": [r.close for r in records],
            "volume": [r.volume for r in records],
            "count": [r.count for r in records],
            "bid": [r.bid for r in records],
            "ask": [r.ask for r in records],
        }

        new_table = pa.Table.from_pydict(data)

        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if parquet_path.exists():
            existing_table = pq.read_table(parquet_path)
            combined = pa.concat_tables([existing_table, new_table])

            # å»é‡ (æŒ‰ symbol, date)
            # DuckDB æ–¹å¼å»é‡
            import duckdb

            combined_df = combined.to_pandas()
            combined_df = combined_df.drop_duplicates(
                subset=["symbol", "date"], keep="last"
            )
            combined_df = combined_df.sort_values(["symbol", "date"])
            new_table = pa.Table.from_pandas(combined_df, preserve_index=False)

        pq.write_table(new_table, parquet_path)
        logger.debug(f"Saved stock data to {parquet_path}")

    # ========== Option Data Download ==========

    def download_options(
        self,
        symbols: list[str],
        start_date: date,
        end_date: date,
        max_dte: int = 90,
        strike_range: int = 30,
        chunk_days: int = 7,
        on_progress: Callable[[str, date, int, int], None] | None = None,
    ) -> dict[str, int]:
        """ä¸‹è½½æœŸæƒå†å²æ•°æ®

        Args:
            symbols: æ ‡çš„ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_dte: æœ€å¤§ DTE è¿‡æ»¤ (é»˜è®¤ 60 å¤©)
            strike_range: ATM ä¸Šä¸‹å„ N ä¸ª strikes (é»˜è®¤ 30)
            chunk_days: æ¯æ¬¡è¯·æ±‚çš„å¤©æ•° (é»˜è®¤ 7ï¼Œä½¿ç”¨ CSV æµå¼è¯»å–)
            on_progress: è¿›åº¦å›è°ƒ (symbol, chunk_start, chunk_idx, total_chunks)

        Returns:
            {symbol: record_count} ä¸‹è½½è®°å½•æ•°

        Note:
            ä½¿ç”¨ expiration=* + max_dte + strike_range çº¦æŸå‡å°‘æ•°æ®é‡ã€‚
            ä½¿ç”¨ CSV æµå¼è¯»å–å¤„ç†å¤§æ•°æ®é‡ï¼Œé¿å… JSON è§£æå¤±è´¥ã€‚
        """
        results = {}

        for symbol in symbols:
            try:
                count = self._download_option(
                    symbol, start_date, end_date, max_dte, strike_range,
                    chunk_days, on_progress
                )
                results[symbol] = count
                logger.info(f"Downloaded {count} option records for {symbol}")
            except Exception as e:
                logger.error(f"Failed to download options for {symbol}: {e}")
                results[symbol] = 0

        # æ›´æ–°æ•°æ®ç›®å½•
        self.update_catalog()

        return results

    def _is_trading_day(self, d: date) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆæ’é™¤å‘¨æœ«ï¼‰"""
        return d.weekday() < 5

    def _download_option(
        self,
        symbol: str,
        start_date: date,
        end_date: date,
        max_dte: int,
        strike_range: int | None,
        chunk_days: int,
        on_progress: Callable[[str, date, int, int], None] | None = None,
    ) -> int:
        """ä¸‹è½½å•åªæ ‡çš„çš„æœŸæƒæ•°æ®

        ä½¿ç”¨ CSV æµå¼è¯»å– + æ—¥æœŸèŒƒå›´åˆ†å—è¯·æ±‚ï¼Œæœ‰æ•ˆå¤„ç†å¤§æ•°æ®é‡ã€‚
        ä½¿ç”¨ expiration=* + max_dte + strike_range çº¦æŸå‡å°‘æ•°æ®é‡ã€‚

        Args:
            symbol: æ ‡çš„ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_dte: æœ€å¤§ DTE è¿‡æ»¤
            strike_range: ATM ä¸Šä¸‹å„ N ä¸ª strikes
            chunk_days: æ¯æ¬¡è¯·æ±‚çš„å¤©æ•° (å»ºè®® 7-14 å¤©)
            on_progress: è¿›åº¦å›è°ƒ
        """
        progress_key = self._get_progress_key(symbol, "option")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„è¿›åº¦
        progress = self._progress.get(progress_key)
        resume_date = start_date

        if progress:
            if progress.status == "completed":
                if progress.start_date <= start_date and progress.end_date >= end_date:
                    logger.info(f"Options for {symbol} already downloaded")
                    return progress.total_records
            elif progress.status == "in_progress" and progress.last_completed_date:
                resume_date = progress.last_completed_date + timedelta(days=1)
                logger.info(f"Resuming {symbol} from {resume_date}")

        # åˆå§‹åŒ–è¿›åº¦
        if not progress or progress.status != "in_progress":
            progress = DownloadProgress(
                symbol=symbol,
                data_type="option",
                start_date=start_date,
                end_date=end_date,
                status="in_progress",
            )
        self._progress[progress_key] = progress
        self._save_progress()

        # ç”Ÿæˆæ—¥æœŸåˆ†å— (æŒ‰ chunk_days åˆ†ç»„)
        chunks: list[tuple[date, date]] = []
        current = resume_date
        while current <= end_date:
            chunk_end = min(current + timedelta(days=chunk_days - 1), end_date)
            # è·³è¿‡çº¯å‘¨æœ«çš„ chunk
            if self._has_trading_day(current, chunk_end):
                chunks.append((current, chunk_end))
            current = chunk_end + timedelta(days=1)

        if not chunks:
            logger.warning(f"No chunks to download for {resume_date} - {end_date}")
            progress.status = "completed"
            self._save_progress()
            return progress.total_records

        total_records = progress.total_records
        total_chunks = len(chunks)

        try:
            for chunk_idx, (chunk_start, chunk_end) in enumerate(chunks):
                if on_progress:
                    on_progress(symbol, chunk_start, chunk_idx, total_chunks)

                try:
                    # ä½¿ç”¨æ—¥æœŸèŒƒå›´è¯·æ±‚ (CSV æµå¼è¯»å–)
                    records = self._client.get_option_with_greeks(
                        symbol=symbol,
                        start_date=chunk_start,
                        end_date=chunk_end,
                        expiration=None,  # expiration=*
                        max_dte=max_dte,
                        strike_range=strike_range,
                    )

                    if records:
                        # æŒ‰å¹´ä»½åˆ†ç»„ä¿å­˜
                        records_by_year: dict[int, list] = {}
                        for r in records:
                            year = r.date.year
                            if year not in records_by_year:
                                records_by_year[year] = []
                            records_by_year[year].append(r)

                        for year, year_records in records_by_year.items():
                            self._save_option_parquet(symbol, year, year_records)

                        total_records += len(records)
                        logger.info(
                            f"{symbol} {chunk_start} - {chunk_end}: {len(records)} contracts"
                        )
                    else:
                        logger.debug(f"{symbol} {chunk_start} - {chunk_end}: No data")

                except Exception as e:
                    logger.warning(f"{symbol} {chunk_start} - {chunk_end}: {e}")

                progress.last_completed_date = chunk_end
                progress.total_records = total_records
                self._save_progress()

            progress.status = "completed"
            self._save_progress()
            logger.info(f"{symbol}: Total {total_records} contracts")

            return total_records

        except Exception as e:
            progress.status = "failed"
            progress.error_message = str(e)
            self._save_progress()
            raise

    def _has_trading_day(self, start: date, end: date) -> bool:
        """æ£€æŸ¥æ—¥æœŸèŒƒå›´å†…æ˜¯å¦æœ‰äº¤æ˜“æ—¥"""
        current = start
        while current <= end:
            if self._is_trading_day(current):
                return True
            current += timedelta(days=1)
        return False

    def _save_option_parquet(
        self,
        symbol: str,
        year: int,
        records: list[OptionEODGreeks],
    ) -> None:
        """ä¿å­˜æœŸæƒæ•°æ®ä¸º Parquet (æŒ‰å¹´ä»½åˆ†æ–‡ä»¶)

        è¿½åŠ æ¨¡å¼ï¼šå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®å¹¶åˆå¹¶å»é‡ã€‚
        """
        parquet_path = get_parquet_path(self._data_dir, "option", symbol, year)
        parquet_path.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ä¸º PyArrow Table
        data = {
            "symbol": [r.symbol for r in records],
            "expiration": [r.expiration for r in records],
            "strike": [r.strike for r in records],
            "option_type": [r.option_type for r in records],
            "date": [r.date for r in records],
            "open": [r.open for r in records],
            "high": [r.high for r in records],
            "low": [r.low for r in records],
            "close": [r.close for r in records],
            "volume": [r.volume for r in records],
            "count": [r.count for r in records],
            "bid": [r.bid for r in records],
            "ask": [r.ask for r in records],
            "delta": [r.delta for r in records],
            "gamma": [r.gamma for r in records],
            "theta": [r.theta for r in records],
            "vega": [r.vega for r in records],
            "rho": [r.rho for r in records],
            "implied_vol": [r.implied_vol for r in records],
            "underlying_price": [r.underlying_price for r in records],
            "open_interest": [r.open_interest for r in records],
            "iv_error": [r.iv_error for r in records],
        }

        new_table = pa.Table.from_pydict(data)

        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if parquet_path.exists():
            existing_table = pq.read_table(parquet_path)
            combined = pa.concat_tables([existing_table, new_table])

            # å»é‡ (æŒ‰ symbol, expiration, strike, option_type, date)
            combined_df = combined.to_pandas()
            combined_df = combined_df.drop_duplicates(
                subset=["symbol", "expiration", "strike", "option_type", "date"],
                keep="last",
            )
            combined_df = combined_df.sort_values(
                ["symbol", "date", "expiration", "strike", "option_type"]
            )
            new_table = pa.Table.from_pandas(combined_df, preserve_index=False)

        pq.write_table(new_table, parquet_path)

    # ========== Utility Methods ==========

    def get_download_status(self) -> dict[str, DownloadProgress]:
        """è·å–æ‰€æœ‰ä¸‹è½½è¿›åº¦"""
        return self._progress.copy()

    def reset_progress(self, symbol: str | None = None, data_type: str | None = None) -> None:
        """é‡ç½®ä¸‹è½½è¿›åº¦

        Args:
            symbol: é‡ç½®ç‰¹å®šæ ‡çš„ (None è¡¨ç¤ºå…¨éƒ¨)
            data_type: é‡ç½®ç‰¹å®šç±»å‹ ("stock" æˆ– "option")
        """
        if symbol is None and data_type is None:
            self._progress = {}
        else:
            keys_to_remove = []
            for key, progress in self._progress.items():
                if symbol and progress.symbol != symbol:
                    continue
                if data_type and progress.data_type != data_type:
                    continue
                keys_to_remove.append(key)

            for key in keys_to_remove:
                del self._progress[key]

        self._save_progress()

    def get_available_symbols(self) -> list[str]:
        """è·å–å·²ä¸‹è½½çš„æ ‡çš„åˆ—è¡¨"""
        option_dir = self._data_dir / "option_daily"
        if option_dir.exists():
            return [d.name for d in option_dir.iterdir() if d.is_dir()]
        return []

    def get_date_range(self, symbol: str) -> tuple[date, date] | None:
        """è·å–æ ‡çš„çš„æ•°æ®æ—¥æœŸèŒƒå›´

        Args:
            symbol: æ ‡çš„ä»£ç 

        Returns:
            (start_date, end_date) æˆ– None
        """
        option_dir = self._data_dir / "option_daily" / symbol.upper()
        if not option_dir.exists():
            return None

        parquet_files = list(option_dir.glob("*.parquet"))
        if not parquet_files:
            return None

        # è¯»å–æ‰€æœ‰å¹´ä»½çš„æ•°æ®ï¼Œæ‰¾åˆ°æ—¥æœŸèŒƒå›´
        all_dates = []
        for pf in parquet_files:
            table = pq.read_table(pf, columns=["date"])
            dates = table["date"].to_pylist()
            all_dates.extend(dates)

        if not all_dates:
            return None

        return min(all_dates), max(all_dates)

    def update_catalog(self) -> dict:
        """æ›´æ–°æ•°æ®ç›®å½•æ–‡ä»¶

        æ‰«ææ‰€æœ‰ Parquet æ–‡ä»¶ï¼Œç”Ÿæˆç»Ÿä¸€çš„æ•°æ®ç›®å½•ã€‚
        ä¿å­˜åˆ° data_catalog.jsonã€‚

        Returns:
            ç›®å½•å­—å…¸
        """
        import duckdb
        from datetime import datetime as dt

        catalog: dict = {
            "updated_at": dt.now().isoformat(),
            "data_dir": str(self._data_dir),
            "datasets": {}
        }

        conn = duckdb.connect(":memory:")

        # 1. Stock Data
        stock_path = self._data_dir / "stock_daily.parquet"
        if stock_path.exists():
            try:
                rows = conn.execute(f"""
                    SELECT symbol,
                           MIN(date) as start_date,
                           MAX(date) as end_date,
                           COUNT(*) as records
                    FROM read_parquet('{stock_path}')
                    GROUP BY symbol
                    ORDER BY symbol
                """).fetchall()

                catalog["datasets"]["stock"] = {
                    "file": "stock_daily.parquet",
                    "symbols": {
                        row[0]: {
                            "start_date": str(row[1]),
                            "end_date": str(row[2]),
                            "records": row[3]
                        }
                        for row in rows
                    }
                }
            except Exception as e:
                logger.warning(f"Failed to scan stock data: {e}")

        # 2. Option Data
        option_dir = self._data_dir / "option_daily"
        if option_dir.exists():
            option_catalog: dict = {"symbols": {}}
            for sym_dir in sorted(option_dir.iterdir()):
                if not sym_dir.is_dir():
                    continue
                files = list(sym_dir.glob("*.parquet"))
                if not files:
                    continue

                try:
                    parquet_list = ", ".join([f"'{f}'" for f in files])
                    result = conn.execute(f"""
                        SELECT MIN(date), MAX(date), COUNT(*)
                        FROM read_parquet([{parquet_list}])
                    """).fetchone()

                    if result:
                        option_catalog["symbols"][sym_dir.name] = {
                            "start_date": str(result[0]),
                            "end_date": str(result[1]),
                            "records": result[2],
                            "files": [f.name for f in files]
                        }
                except Exception as e:
                    logger.warning(f"Failed to scan option data for {sym_dir.name}: {e}")

            if option_catalog["symbols"]:
                catalog["datasets"]["option"] = option_catalog

        # 3. Macro Data
        macro_path = self._data_dir / "macro_daily.parquet"
        if macro_path.exists():
            try:
                rows = conn.execute(f"""
                    SELECT indicator,
                           MIN(date) as start_date,
                           MAX(date) as end_date,
                           COUNT(*) as records
                    FROM read_parquet('{macro_path}')
                    GROUP BY indicator
                    ORDER BY indicator
                """).fetchall()

                catalog["datasets"]["macro"] = {
                    "file": "macro_daily.parquet",
                    "indicators": {
                        row[0]: {
                            "start_date": str(row[1]),
                            "end_date": str(row[2]),
                            "records": row[3]
                        }
                        for row in rows
                    }
                }
            except Exception as e:
                logger.warning(f"Failed to scan macro data: {e}")

        # 4. Fundamental Data
        for data_type in ["eps", "revenue", "dividend"]:
            path = self._data_dir / f"fundamental_{data_type}.parquet"
            if path.exists():
                try:
                    rows = conn.execute(f"""
                        SELECT symbol,
                               MIN(as_of_date) as start_date,
                               MAX(as_of_date) as end_date,
                               COUNT(*) as records
                        FROM read_parquet('{path}')
                        GROUP BY symbol
                        ORDER BY symbol
                    """).fetchall()

                    if f"fundamental_{data_type}" not in catalog["datasets"]:
                        catalog["datasets"][f"fundamental_{data_type}"] = {
                            "file": f"fundamental_{data_type}.parquet",
                            "symbols": {}
                        }

                    for row in rows:
                        catalog["datasets"][f"fundamental_{data_type}"]["symbols"][row[0]] = {
                            "start_date": str(row[1]),
                            "end_date": str(row[2]),
                            "records": row[3]
                        }
                except Exception as e:
                    logger.warning(f"Failed to scan fundamental {data_type} data: {e}")

        # ä¿å­˜ç›®å½•æ–‡ä»¶
        catalog_path = self._data_dir / "data_catalog.json"
        try:
            with open(catalog_path, "w") as f:
                json.dump(catalog, f, indent=2, default=str)
            logger.info(f"Updated data catalog: {catalog_path}")
        except Exception as e:
            logger.warning(f"Failed to save catalog: {e}")

        return catalog

    def print_catalog(self) -> None:
        """æ‰“å°æ•°æ®ç›®å½•æ‘˜è¦"""
        catalog_path = self._data_dir / "data_catalog.json"

        if not catalog_path.exists():
            self.update_catalog()

        try:
            with open(catalog_path) as f:
                catalog = json.load(f)
        except Exception as e:
            logger.error(f"Failed to read catalog: {e}")
            return

        print("=" * 60)
        print(f"ğŸ“Š Data Catalog (updated: {catalog.get('updated_at', 'N/A')})")
        print("=" * 60)

        datasets = catalog.get("datasets", {})

        # Stock
        if "stock" in datasets:
            print("\nğŸ“ˆ Stock Data:")
            for sym, info in datasets["stock"].get("symbols", {}).items():
                print(f"   {sym}: {info['start_date']} ~ {info['end_date']} ({info['records']} days)")

        # Option
        if "option" in datasets:
            print("\nğŸ“Š Option Data:")
            for sym, info in datasets["option"].get("symbols", {}).items():
                print(f"   {sym}: {info['start_date']} ~ {info['end_date']} ({info['records']} records)")

        # Macro
        if "macro" in datasets:
            print("\nğŸŒ Macro Data:")
            for ind, info in datasets["macro"].get("indicators", {}).items():
                print(f"   {ind}: {info['start_date']} ~ {info['end_date']} ({info['records']} days)")

        # Fundamental
        for data_type in ["eps", "revenue", "dividend"]:
            key = f"fundamental_{data_type}"
            if key in datasets:
                print(f"\nğŸ“‘ Fundamental {data_type.upper()}:")
                for sym, info in datasets[key].get("symbols", {}).items():
                    print(f"   {sym}: {info['start_date']} ~ {info['end_date']} ({info['records']} records)")

        print("\n" + "=" * 60)

    # ========== Incremental Download Methods ==========

    def download_stocks_incremental(
        self,
        gaps: list,  # list[DataGap] from data_checker
        on_progress: Callable[[str, int, int], None] | None = None,
    ) -> dict[str, int]:
        """æ ¹æ®æ•°æ®ç¼ºå£å¢é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®

        Args:
            gaps: DataChecker.check_stock_gaps() è¿”å›çš„ç¼ºå£åˆ—è¡¨
            on_progress: è¿›åº¦å›è°ƒ (symbol, current, total)

        Returns:
            {symbol: downloaded_records}
        """
        if not gaps:
            logger.info("No stock data gaps to download")
            return {}

        results: dict[str, int] = {}
        total = len(gaps)

        for i, gap in enumerate(gaps):
            if on_progress:
                on_progress(gap.symbol, i, total)

            logger.info(
                f"Downloading stock {gap.symbol} "
                f"{gap.missing_start} ~ {gap.missing_end} ({gap.reason})"
            )

            try:
                # ç›´æ¥è°ƒç”¨åº•å±‚ä¸‹è½½æ–¹æ³•ï¼ˆä¸èµ°è¿›åº¦æ£€æŸ¥ï¼‰
                records = self._client.get_stock_eod(
                    gap.symbol,
                    gap.missing_start,
                    gap.missing_end,
                )

                if records:
                    self._save_stock_parquet(gap.symbol, records)
                    count = len(records)
                    results[gap.symbol] = results.get(gap.symbol, 0) + count
                    logger.info(f"Downloaded {count} stock records for {gap.symbol}")

                    # æ›´æ–°è¿›åº¦ï¼ˆæ‰©å±•ç°æœ‰èŒƒå›´ï¼‰
                    self._update_progress_range(
                        gap.symbol,
                        "stock",
                        gap.missing_start,
                        gap.missing_end,
                        count,
                    )

            except Exception as e:
                logger.error(f"Failed to download stock {gap.symbol}: {e}")

        # æ›´æ–°æ•°æ®ç›®å½•
        if results:
            self.update_catalog()

        return results

    def download_options_incremental(
        self,
        gaps: list,  # list[DataGap] from data_checker
        max_dte: int = 90,
        strike_range: int = 30,
        chunk_days: int = 7,
        on_progress: Callable[[str, date, int, int], None] | None = None,
    ) -> dict[str, int]:
        """æ ¹æ®æ•°æ®ç¼ºå£å¢é‡ä¸‹è½½æœŸæƒæ•°æ®

        Args:
            gaps: DataChecker.check_option_gaps() è¿”å›çš„ç¼ºå£åˆ—è¡¨
            max_dte: æœ€å¤§ DTE
            strike_range: ATM ä¸Šä¸‹å„ N ä¸ª strikes
            chunk_days: æ¯æ¬¡è¯·æ±‚çš„å¤©æ•°
            on_progress: è¿›åº¦å›è°ƒ

        Returns:
            {symbol: downloaded_records}
        """
        if not gaps:
            logger.info("No option data gaps to download")
            return {}

        results: dict[str, int] = {}

        for gap in gaps:
            logger.info(
                f"Downloading option {gap.symbol} "
                f"{gap.missing_start} ~ {gap.missing_end} ({gap.reason})"
            )

            try:
                count = self._download_option(
                    gap.symbol,
                    gap.missing_start,
                    gap.missing_end,
                    max_dte,
                    strike_range,
                    chunk_days,
                    on_progress,
                )
                results[gap.symbol] = results.get(gap.symbol, 0) + count
                logger.info(f"Downloaded {count} option records for {gap.symbol}")

            except Exception as e:
                logger.error(f"Failed to download option {gap.symbol}: {e}")

        # æ›´æ–°æ•°æ®ç›®å½•
        if results:
            self.update_catalog()

        return results

    def _update_progress_range(
        self,
        symbol: str,
        data_type: str,
        new_start: date,
        new_end: date,
        records: int,
    ) -> None:
        """æ›´æ–°è¿›åº¦è®°å½•çš„æ—¥æœŸèŒƒå›´ï¼ˆæ‰©å±•æ¨¡å¼ï¼‰

        Args:
            symbol: æ ‡çš„ä»£ç 
            data_type: æ•°æ®ç±»å‹
            new_start: æ–°ä¸‹è½½çš„å¼€å§‹æ—¥æœŸ
            new_end: æ–°ä¸‹è½½çš„ç»“æŸæ—¥æœŸ
            records: æ–°ä¸‹è½½çš„è®°å½•æ•°
        """
        progress_key = self._get_progress_key(symbol, data_type)
        existing = self._progress.get(progress_key)

        if existing and existing.status == "completed":
            # æ‰©å±•ç°æœ‰èŒƒå›´
            updated_start = min(existing.start_date, new_start)
            updated_end = max(existing.end_date, new_end)
            updated_records = existing.total_records + records

            self._progress[progress_key] = DownloadProgress(
                symbol=symbol,
                data_type=data_type,
                start_date=updated_start,
                end_date=updated_end,
                last_completed_date=updated_end,
                total_records=updated_records,
                status="completed",
            )
        else:
            # åˆ›å»ºæ–°è®°å½•
            self._progress[progress_key] = DownloadProgress(
                symbol=symbol,
                data_type=data_type,
                start_date=new_start,
                end_date=new_end,
                last_completed_date=new_end,
                total_records=records,
                status="completed",
            )

        self._save_progress()
        logger.debug(
            f"Updated progress for {data_type}:{symbol}: "
            f"{self._progress[progress_key].start_date} ~ "
            f"{self._progress[progress_key].end_date}"
        )
